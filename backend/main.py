#!/usr/bin/env python3
"""
Docusaurus → Cohere → Qdrant Ingestion Pipeline

This script ingests content from deployed Docusaurus book URLs, generates semantic embeddings using Cohere,
and stores them in Qdrant with proper metadata for future retrieval.
"""

import asyncio
import hashlib
import logging
import time
import sys
import argparse
from datetime import datetime
from typing import List, Dict, Optional, Any
from dataclasses import dataclass, field
from urllib.parse import urljoin, urlparse
import os
import re

import requests
from bs4 import BeautifulSoup
import cohere
from qdrant_client import QdrantClient
from qdrant_client.http.models import PointStruct, VectorParams, Distance
from dotenv import load_dotenv
import pydantic


# Load environment variables from .env file
import os
from pathlib import Path

# Load from current directory first
env_path = Path('.env').resolve()
if env_path.exists():
    load_dotenv(str(env_path))
    print(f"Loaded .env from: {env_path}")

# Also try to load from parent directory if not found in current directory
parent_env_path = Path('../.env').resolve()
if not os.getenv("COHERE_API_KEY") and parent_env_path.exists():
    load_dotenv(str(parent_env_path))
    print(f"Loaded .env from: {parent_env_path}")

# And try the project root directory
root_env_path = Path('../../.env').resolve()
if not os.getenv("COHERE_API_KEY") and root_env_path.exists():
    load_dotenv(str(root_env_path))
    print(f"Loaded .env from: {root_env_path}")


@dataclass
class Configuration:
    """Runtime configuration for the ingestion pipeline."""
    cohere_model: str = "embed-english-v3.0"
    chunk_size: int = 1000
    chunk_overlap: int = 100
    qdrant_collection_name: str = "book_content_chunks"
    vector_dimension: int = 1024  # Default for Cohere multilingual v3
    request_timeout: int = 30
    retry_attempts: int = 3
    concurrent_workers: int = 5

    def __post_init__(self):
        # Override defaults with environment variables if they exist
        self.cohere_model = os.getenv("COHERE_MODEL", self.cohere_model)
        self.chunk_size = int(os.getenv("CHUNK_SIZE", str(self.chunk_size)))
        self.chunk_overlap = int(os.getenv("CHUNK_OVERLAP", str(self.chunk_overlap)))
        self.qdrant_collection_name = os.getenv("QDRANT_COLLECTION_NAME", self.qdrant_collection_name)
        self.request_timeout = int(os.getenv("REQUEST_TIMEOUT", str(self.request_timeout)))
        self.retry_attempts = int(os.getenv("RETRY_ATTEMPTS", str(self.retry_attempts)))
        self.concurrent_workers = int(os.getenv("CONCURRENT_WORKERS", str(self.concurrent_workers)))

        # Validate configuration values
        self.validate()

    def validate(self):
        """Validate configuration values."""
        errors = []

        if self.chunk_size <= 0:
            errors.append("CHUNK_SIZE must be greater than 0")
        if self.chunk_overlap < 0:
            errors.append("CHUNK_OVERLAP must be greater than or equal to 0")
        if self.chunk_overlap >= self.chunk_size:
            errors.append("CHUNK_OVERLAP must be less than CHUNK_SIZE")
        if self.request_timeout <= 0:
            errors.append("REQUEST_TIMEOUT must be greater than 0")
        if self.retry_attempts < 0:
            errors.append("RETRY_ATTEMPTS must be greater than or equal to 0")
        if self.concurrent_workers <= 0:
            errors.append("CONCURRENT_WORKERS must be greater than 0")

        if errors:
            raise ValueError(f"Invalid configuration: {'; '.join(errors)}")


@dataclass
class ContentChunk:
    """Represents a segment of text extracted from a webpage."""
    text: str
    url: str
    title: str
    section_heading: str = ""
    chunk_index: int = 0
    content_hash: str = ""
    word_count: int = 0
    char_count: int = 0

    def __post_init__(self):
        if not self.content_hash:
            self.content_hash = self._generate_content_hash()
        if self.word_count == 0:
            self.word_count = len(self.text.split())
        if self.char_count == 0:
            self.char_count = len(self.text)

    def _generate_content_hash(self) -> str:
        """Generate SHA256 hash of the content for idempotency."""
        content = f"{self.url}|{self.text}|{self.chunk_index}"
        return hashlib.sha256(content.encode()).hexdigest()


@dataclass
class EmbeddingVector:
    """Semantic representation of content chunk text generated by Cohere model."""
    id: int  # Changed from str to int to match Qdrant requirements
    vector: List[float]
    payload: Dict[str, Any]

    def __post_init__(self):
        if not self.id and "content_hash" in self.payload:
            # Convert content_hash to integer ID if available in payload
            content_hash = self.payload["content_hash"]
            # Take first 16 hex chars and convert to int
            self.id = int(content_hash[:16], 16)


@dataclass
class IngestionJob:
    """Represents a single execution of the ingestion pipeline."""
    job_id: str = ""
    start_time: datetime = field(default_factory=datetime.now)
    end_time: Optional[datetime] = None
    status: str = "pending"  # pending, running, completed, failed
    total_urls_discovered: int = 0
    urls_processed: int = 0
    chunks_created: int = 0
    vectors_stored: int = 0
    errors: List[Dict] = field(default_factory=list)
    processing_stats: Dict[str, float] = field(default_factory=dict)

    def __post_init__(self):
        if not self.job_id:
            self.job_id = f"job_{self.start_time.timestamp()}"

    def calculate_performance_metrics(self):
        """Calculate performance metrics for the job."""
        if self.end_time and self.start_time:
            processing_time = (self.end_time - self.start_time).total_seconds()
            self.processing_stats["total_processing_time"] = processing_time

            if processing_time > 0:
                self.processing_stats["urls_per_second"] = self.urls_processed / processing_time
                self.processing_stats["chunks_per_second"] = self.chunks_created / processing_time
                self.processing_stats["vectors_per_second"] = self.vectors_stored / processing_time

            if self.total_urls_discovered > 0:
                self.processing_stats["url_success_rate"] = (self.urls_processed / self.total_urls_discovered) * 100
            if self.chunks_created > 0:
                self.processing_stats["vector_success_rate"] = (self.vectors_stored / self.chunks_created) * 100

    def complete(self):
        """Mark the job as completed."""
        self.end_time = datetime.now()
        self.status = "completed"

        # Calculate performance metrics
        self.calculate_performance_metrics()


def setup_logging():
    """Set up logging configuration with structured output."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )


def retry_on_failure(func, max_retries=3, delay=1, backoff=2):
    """
    Retry decorator to handle transient failures.

    Args:
        func: Function to retry
        max_retries: Maximum number of retry attempts
        delay: Initial delay between retries
        backoff: Multiplier for delay after each retry

    Returns:
        Result of the function call
    """
    def wrapper(*args, **kwargs):
        current_delay = delay
        last_exception = None

        for attempt in range(max_retries + 1):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                last_exception = e
                if attempt < max_retries:
                    logging.warning(f"Attempt {attempt + 1} failed: {str(e)}. Retrying in {current_delay} seconds...")
                    time.sleep(current_delay)
                    current_delay *= backoff
                else:
                    logging.error(f"All {max_retries + 1} attempts failed. Last error: {str(e)}")

        raise last_exception

    return wrapper


def main() -> Dict[str, Any]:
    """
    Execute the complete ingestion pipeline: discovers URLs from a Docusaurus site,
    extracts content, generates embeddings using Cohere, and stores them in Qdrant.

    Returns:
        A dictionary containing ingestion statistics
    """
    start_time = datetime.now()
    setup_logging()
    logger = logging.getLogger(__name__)

    # Validate environment variables
    required_vars = ["COHERE_API_KEY", "QDRANT_URL", "QDRANT_API_KEY", "DOCS_URL"]
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    if missing_vars:
        # Check if DEPLOYED_PROJECT_URL is available and use it as backup
        if "DOCS_URL" in missing_vars and os.getenv("DEPLOYED_PROJECT_URL"):
            # This is fine, we'll handle it in the next section
            missing_vars.remove("DOCS_URL")
        if missing_vars:
            raise ValueError(f"Required environment variables not set: {', '.join(missing_vars)}")

    # Use DEPLOYED_PROJECT_URL if DOCS_URL is not set
    if not os.getenv("DOCS_URL"):
        deployed_url = os.getenv("DEPLOYED_PROJECT_URL")
        if deployed_url:
            os.environ["DOCS_URL"] = deployed_url
            logging.info(f"Using DEPLOYED_PROJECT_URL as DOCS_URL: {deployed_url}")
        else:
            # If neither is set, use a default URL for testing
            os.environ["DOCS_URL"] = "https://physical-ai-and-humanoid-robotics-b-sooty.vercel.app"
            logging.info(f"Using default URL as DOCS_URL: {os.environ['DOCS_URL']}")

    # Initialize configuration
    config = Configuration()
    logger.info("Starting ingestion pipeline with configuration: %s", config)

    # Initialize clients with error handling
    try:
        cohere_client = cohere.Client(os.getenv("COHERE_API_KEY"))
        logger.info("Cohere client initialized successfully")
    except Exception as e:
        raise ValueError(f"Failed to initialize Cohere client: {str(e)}")

    qdrant_url = os.getenv("QDRANT_URL")
    qdrant_api_key = os.getenv("QDRANT_API_KEY")

    # Determine if using local or cloud Qdrant
    try:
        if "localhost" in qdrant_url or "127.0.0.1" in qdrant_url:
            # Local Qdrant
            host_match = re.match(r'(?:https?://)?([^:/]+)(?::(\d+))?', qdrant_url)
            if host_match:
                host = host_match.group(1)
                port_str = host_match.group(2)
                port = int(port_str) if port_str else 6333  # Default Qdrant port
                qdrant_client = QdrantClient(host=host, port=port)
            else:
                qdrant_client = QdrantClient(url=qdrant_url, api_key=qdrant_api_key)
        else:
            # Qdrant Cloud
            qdrant_client = QdrantClient(url=qdrant_url, api_key=qdrant_api_key)

        # Test Qdrant connection
        qdrant_client.get_collections()
        logger.info("Qdrant client initialized and connection verified successfully")
    except Exception as e:
        raise ValueError(f"Failed to initialize Qdrant client: {str(e)}")

    # Create or ensure collection exists
    try:
        qdrant_client.get_collection(config.qdrant_collection_name)
        logger.info("Using existing Qdrant collection: %s", config.qdrant_collection_name)
    except:
        qdrant_client.create_collection(
            collection_name=config.qdrant_collection_name,
            vectors_config=VectorParams(size=config.vector_dimension, distance=Distance.COSINE),
        )
        logger.info("Created new Qdrant collection: %s", config.qdrant_collection_name)

    # Initialize ingestion job
    job = IngestionJob()
    job.status = "running"

    try:
        # Discover URLs from the Docusaurus site
        docs_url = os.getenv("DOCS_URL")
        if not docs_url:
            raise ValueError("DOCS_URL environment variable is required")

        logger.info("Discovering URLs from: %s", docs_url)
        urls = discover_urls_from_sitemap(docs_url)
        job.total_urls_discovered = len(urls)
        logger.info("Discovered %d URLs", len(urls))

        # Process each URL with retry logic
        for url in urls:
            try:
                logger.info("Processing URL: %s", url)

                # Extract content from URL (with manual retry to avoid lambda issues)
                content_chunk = None
                for attempt in range(config.retry_attempts + 1):
                    try:
                        content_chunk = extract_content_from_url(url)
                        break  # Success, exit retry loop
                    except Exception as e:
                        if attempt < config.retry_attempts:
                            logger.warning(f"Content extraction attempt {attempt + 1} failed: {str(e)}. Retrying...")
                            time.sleep(min(2 ** attempt, 10))  # Exponential backoff with max 10s
                        else:
                            logger.error(f"All content extraction attempts failed for URL {url}: {str(e)}")
                            # Create a minimal chunk with error info to continue processing
                            content_chunk = ContentChunk(text="", url=url, title="Error extracting content")
                            break

                # Check if content is empty before chunking
                if not content_chunk.text.strip():
                    logger.warning(f"No content extracted from URL: {url}")
                    continue

                # Chunk the content
                chunks = chunk_text(content_chunk.text, config.chunk_size, config.chunk_overlap)

                # Check if chunks list is empty
                if not chunks:
                    logger.warning(f"No chunks created from content in URL: {url}")
                    continue

                # Process each chunk
                for i, chunk_text_content in enumerate(chunks):
                    # Create content chunk
                    chunk = ContentChunk(
                        text=chunk_text_content,
                        url=url,
                        title=content_chunk.title,
                        section_heading=content_chunk.section_heading,
                        chunk_index=i
                    )

                    # Generate embedding with retry - avoid lambda closure issues
                    embedding_success = False
                    for attempt in range(config.retry_attempts + 1):
                        try:
                            embedding_response = cohere_client.embed(
                                texts=[chunk.text],
                                model=config.cohere_model,
                                input_type="search_document"
                            )

                            embedding = embedding_response.embeddings[0]

                            # Create embedding vector - convert hash to integer for Qdrant compatibility
                            # Use the first 8 hex characters of the hash and convert to int
                            qdrant_id = int(chunk.content_hash[:16], 16)  # Use first 16 hex chars (64 bits) to create integer ID
                            embedding_vector = EmbeddingVector(
                                id=qdrant_id,  # Pass as integer as Qdrant expects integer IDs
                                vector=embedding,
                                payload={
                                    "url": chunk.url,
                                    "page_title": chunk.title,
                                    "section_heading": chunk.section_heading,
                                    "chunk_index": chunk.chunk_index,
                                    "text": chunk.text,
                                    "content_hash": chunk.content_hash
                                }
                            )

                            # Store in Qdrant with retry
                            for q_attempt in range(config.retry_attempts + 1):
                                try:
                                    qdrant_client.upsert(
                                        collection_name=config.qdrant_collection_name,
                                        points=[
                                            PointStruct(
                                                id=embedding_vector.id,
                                                vector=embedding_vector.vector,
                                                payload=embedding_vector.payload
                                            )
                                        ]
                                    )
                                    break  # Success, exit retry loop
                                except Exception as q_err:
                                    if q_attempt < config.retry_attempts:
                                        logger.warning(f"Qdrant upsert attempt {q_attempt + 1} failed: {str(q_err)}. Retrying...")
                                        time.sleep(min(2 ** q_attempt, 10))  # Exponential backoff with max 10s
                                    else:
                                        raise q_err

                            job.chunks_created += 1
                            job.vectors_stored += 1
                            embedding_success = True
                            break  # Success, exit retry loop
                        except Exception as e:
                            if attempt < config.retry_attempts:
                                logger.warning(f"Embedding attempt {attempt + 1} failed: {str(e)}. Retrying...")
                                time.sleep(min(2 ** attempt, 10))  # Exponential backoff with max 10s
                            else:
                                logger.error(f"All embedding attempts failed for URL {url}: {str(e)}")

                    if not embedding_success:
                        continue  # Skip to next chunk if all attempts failed

                job.urls_processed += 1
                logger.info("Successfully processed URL: %s", url)

            except Exception as e:
                error_info = {
                    "url": url,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat()
                }
                job.errors.append(error_info)
                logger.error("Error processing URL %s: %s", url, str(e))

        # Validate that stored embeddings are queryable
        logger.info("Validating stored embeddings are queryable...")
        validation_result = validate_embeddings_queryable(qdrant_client, config.qdrant_collection_name, job.vectors_stored)
        if validation_result["success"]:
            logger.info(f"Embedding validation successful: {validation_result['message']}")
        else:
            logger.warning(f"Embedding validation failed: {validation_result['message']}")
            # Add to errors but don't fail the job
            job.errors.append({
                "validation_error": validation_result["message"],
                "timestamp": datetime.now().isoformat()
            })

        job.complete()
        logger.info("Ingestion completed. Job stats: %s", {
            "status": job.status,
            "total_urls_discovered": job.total_urls_discovered,
            "urls_processed": job.urls_processed,
            "chunks_created": job.chunks_created,
            "vectors_stored": job.vectors_stored,
            "errors": len(job.errors),
            "performance_metrics": job.processing_stats
        })

        return {
            "status": job.status,
            "total_urls_discovered": job.total_urls_discovered,
            "urls_processed": job.urls_processed,
            "chunks_created": job.chunks_created,
            "vectors_stored": job.vectors_stored,
            "processing_time": (job.end_time - job.start_time).total_seconds() if job.end_time else 0,
            "errors": job.errors,
            "qdrant_collection": config.qdrant_collection_name,
            "performance_metrics": job.processing_stats
        }

    except Exception as e:
        job.status = "failed"
        job.end_time = datetime.now()
        error_info = {
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }
        job.errors.append(error_info)
        logger.error("Ingestion failed: %s", str(e))

        return {
            "status": job.status,
            "total_urls_discovered": job.total_urls_discovered,
            "urls_processed": job.urls_processed,
            "chunks_created": job.chunks_created,
            "vectors_stored": job.vectors_stored,
            "processing_time": (job.end_time - job.start_time).total_seconds() if job.end_time else 0,
            "errors": job.errors,
            "qdrant_collection": config.qdrant_collection_name,
            "performance_metrics": job.processing_stats
        }


def discover_urls_from_sitemap(base_url: str) -> List[str]:
    """
    Discover URLs from sitemap.xml of a Docusaurus site.
    First tries the main sitemap at the root, then falls back to the provided URL.

    Args:
        base_url: Base URL of the Docusaurus site

    Returns:
        List of discovered URLs
    """
    # Try the main sitemap at the root of the domain first
    from urllib.parse import urlparse
    parsed_url = urlparse(base_url)
    root_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
    sitemap_url = f"{root_url}/sitemap.xml"

    logging.info("Fetching sitemap from: %s", sitemap_url)

    try:
        response = requests.get(sitemap_url, timeout=Configuration().request_timeout)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'xml')
        urls = []

        for loc in soup.find_all('loc'):
            url = loc.text.strip()
            # Only include URLs that are part of the original base_url domain/path
            if url.startswith(root_url):
                urls.append(url)

        if urls:
            logging.info(f"Found {len(urls)} URLs from main sitemap")
            return urls
        else:
            # If sitemap exists but has no URLs matching our base, fall back
            logging.warning("Sitemap exists but no matching URLs found, falling back to provided URL")
            return [base_url]
    except Exception as e:
        logging.warning("Could not fetch sitemap from %s: %s. Falling back to provided URL.", sitemap_url, str(e))

        # Try the sitemap at the provided URL level as backup
        backup_sitemap_url = f"{base_url.rstrip('/')}/sitemap.xml"
        try:
            response = requests.get(backup_sitemap_url, timeout=Configuration().request_timeout)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'xml')
            urls = []

            for loc in soup.find_all('loc'):
                url = loc.text.strip()
                if url.startswith(base_url):
                    urls.append(url)

            if urls:
                logging.info(f"Found {len(urls)} URLs from backup sitemap")
                return urls
        except:
            pass  # Ignore errors from backup sitemap attempt

        # If sitemap is not available, return just the base URL
        return [base_url]


def extract_content_from_url(url: str) -> ContentChunk:
    """
    Fetches a web page and extracts clean text content while preserving semantic structure.

    Args:
        url: URL to extract content from

    Returns:
        ContentChunk object with the extracted text and metadata
    """
    config = Configuration()

    # Try multiple times with different headers in case of blocking
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
    }

    try:
        response = requests.get(url, timeout=config.request_timeout, headers=headers)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        logging.warning(f"Could not fetch URL {url}: {str(e)}")
        return ContentChunk(text="", url=url, title="No Title")

    soup = BeautifulSoup(response.content, 'html.parser')

    # Extract title
    title_tag = soup.find('title')
    title = title_tag.get_text().strip() if title_tag else "No Title"

    # Remove unwanted elements that are not content FIRST, before looking for content
    unwanted_selectors = [
        'nav', 'header', 'footer', 'script', 'style', 'noscript',
        '.nav', '.navbar', '.menu', '.sidebar', '.toc', '.table-of-contents',
        '.theme-edit-this-page', '.theme-last-updated', '.pagination-nav',
        '.theme-doc-footer', '.theme-admonition', '.theme-admonition-note',
        '.theme-admonition-tip', '.theme-admonition-info', '.theme-admonition-caution',
        '.theme-doc-sidebar-container', '.docsearch', '.search-bar'
    ]

    for selector in unwanted_selectors:
        for element in soup.select(selector):
            element.decompose()

    # For Docusaurus sites, look for specific content selectors in order of preference
    content_selectors = [
        '[data-testid="doc-root"]',      # Docusaurus specific test ID
        '[data-testid="doc-page"]',      # Another Docusaurus specific ID
        '.doc-item-container',            # Docusaurus container
        '.theme-doc-markdown',            # Docusaurus markdown content
        '.markdown',                      # Common markdown class
        '.theme-doc-content',             # Docusaurus content area
        '.doc-content',                   # Alternative doc content
        'article',                        # Article content
        'main',                           # Main content area
        '[role="main"]',                  # ARIA main role
        '.main-wrapper',                  # Common wrapper
        '.container',                     # General container
        '.content',                       # Generic content
        'div[class*="doc"]',              # Divs with doc in class name
        'div[class*="markdown"]',         # Divs with markdown in class name
        'div[class*="content"]'           # Divs with content in class name
    ]

    content_element = None
    for selector in content_selectors:
        content_element = soup.select_one(selector)
        if content_element:
            break

    # If no specific content found, try to find the largest content block
    if not content_element:
        # Find all divs, articles, sections, and main content containers
        possible_elements = soup.find_all(['div', 'article', 'section', 'main'])
        # Sort by text length to find the most content-rich element
        def get_text_length(element):
            return len(element.get_text(strip=True))
        possible_elements = sorted(possible_elements, key=get_text_length, reverse=True)

        # Take the first element with substantial content (> 100 characters)
        for element in possible_elements:
            if len(element.get_text(strip=True)) > 100:
                content_element = element
                break

    # If still no content found, fall back to body
    if not content_element:
        content_element = soup.find('body')

    # Get clean text content
    text = ""
    if content_element:
        # Get text, but remove excessive whitespace and normalize
        text = content_element.get_text(separator=' ', strip=True)
        # Clean up the text
        import re
        # Remove multiple consecutive whitespace
        text = re.sub(r'\s+', ' ', text).strip()

    # If text is still too short, try to get content from all paragraph-like elements
    if len(text) < 50:
        # Try to get content from headings and paragraphs
        content_parts = []
        for tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'li', 'td', 'blockquote']:
            elements = soup.find_all(tag)
            for elem in elements:
                content = elem.get_text(strip=True)
                if content and len(content) > 10:  # Only add substantial content
                    content_parts.append(content)

        if content_parts:
            text = ' '.join(content_parts)
        else:
            # If still no content, try to get all text from body
            body = soup.find('body')
            if body:
                text = body.get_text(separator=' ', strip=True)[:1000]  # Limit to first 1000 chars

    # Ensure text is a string and not None
    text = text if text else ""

    return ContentChunk(
        text=text,
        url=url,
        title=title
    )


def chunk_text(text: str, chunk_size: int, overlap: int) -> List[str]:
    """
    Chunk text into segments of specified size with overlap, respecting semantic boundaries.

    Args:
        text: Text to chunk
        chunk_size: Maximum size of each chunk
        overlap: Overlap between adjacent chunks

    Returns:
        List of text chunks
    """
    if not text or len(text.strip()) == 0:
        return []  # Return empty list if text is empty

    if len(text) <= chunk_size:
        return [text]

    chunks = []
    start = 0

    while start < len(text):
        # Calculate the end position
        end = start + chunk_size

        # If we're at the end, include the remainder
        if end >= len(text):
            remaining_text = text[start:]
            if remaining_text.strip():  # Only add if not just whitespace
                chunks.append(remaining_text)
            break

        # Define sentence and paragraph separators to look for
        separators = ['\n\n', '\n', '. ', '! ', '? ', '; ', ': ']

        # Look for the best semantic boundary before the chunk size limit
        best_break = start + chunk_size
        found_boundary = False

        # Search for semantic boundaries in reverse order (closest to the limit)
        for separator in separators:
            sep_len = len(separator)
            pos = text.rfind(separator, start, best_break)

            if pos != -1 and pos > start + chunk_size // 2:  # Only if it's reasonably far in
                best_break = pos + sep_len
                found_boundary = True
                break

        # If no good boundary found, try to break at word boundary
        if not found_boundary:
            last_space = text.rfind(' ', start, start + chunk_size)
            if last_space > start + chunk_size // 2:
                best_break = last_space
                found_boundary = True

        # Create the chunk
        chunk = text[start:best_break]
        if chunk.strip():  # Only add if not just whitespace
            chunks.append(chunk)

        # Move start position with overlap
        start = best_break - overlap if overlap > 0 else best_break

    return chunks


def validate_embeddings_queryable(qdrant_client, collection_name: str, expected_count: int) -> Dict[str, Any]:
    """
    Validate that stored embeddings are queryable in Qdrant.

    Args:
        qdrant_client: Initialized Qdrant client
        collection_name: Name of the collection to validate
        expected_count: Expected number of vectors stored

    Returns:
        Dictionary with validation results
    """
    try:
        # Get collection info
        collection_info = qdrant_client.get_collection(collection_name)
        actual_count = collection_info.points_count

        # Check if the count matches expectations
        if actual_count != expected_count:
            return {
                "success": False,
                "message": f"Vector count mismatch: expected {expected_count}, got {actual_count}"
            }

        # Try to query with a simple test vector to ensure vectors are queryable
        if actual_count > 0:
            # Get a sample point to test query functionality
            points = qdrant_client.scroll(
                collection_name=collection_name,
                limit=1
            )[0]

            if points:
                # Try to perform a simple search to validate queryability
                test_point = points[0]
                if hasattr(test_point, 'vector'):
                    # Perform a search with the vector to make sure it works
                    search_results = qdrant_client.search(
                        collection_name=collection_name,
                        query_vector=test_point.vector,
                        limit=1
                    )

                    if len(search_results) > 0:
                        return {
                            "success": True,
                            "message": f"Successfully validated {actual_count} vectors are queryable"
                        }
                    else:
                        return {
                            "success": False,
                            "message": "Vectors exist but search functionality failed"
                        }

        # If no points were found but expected count is 0, that's valid
        if expected_count == 0:
            return {
                "success": True,
                "message": "No vectors expected and none found, validation passed"
            }

        return {
            "success": False,
            "message": "Could not validate vectors - no points to test with"
        }

    except Exception as e:
        return {
            "success": False,
            "message": f"Validation failed with error: {str(e)}"
        }


def create_cli_parser():
    """Create command-line argument parser."""
    parser = argparse.ArgumentParser(description="Docusaurus → Cohere → Qdrant Ingestion Pipeline")
    parser.add_argument(
        "--docs-url",
        type=str,
        help="Base URL of the Docusaurus site to ingest (overrides DOCS_URL env var)"
    )
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=1000,
        help="Maximum size of text chunks in characters (default: 1000)"
    )
    parser.add_argument(
        "--chunk-overlap",
        type=int,
        default=100,
        help="Overlap between adjacent chunks in characters (default: 100)"
    )
    parser.add_argument(
        "--collection-name",
        type=str,
        default="book_content_chunks",
        help="Name of the Qdrant collection (default: book_content_chunks)"
    )
    parser.add_argument(
        "--cohere-model",
        type=str,
        default="embed-english-v3.0",
        help="Cohere embedding model to use (default: embed-english-v3.0)"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Enable verbose logging"
    )
    return parser


def main_with_args(args=None):
    """Main function with optional command-line arguments."""
    parser = create_cli_parser()
    parsed_args = parser.parse_args(args)

    # Override environment variables with command-line arguments if provided
    if parsed_args.docs_url:
        os.environ["DOCS_URL"] = parsed_args.docs_url
    if parsed_args.chunk_size:
        os.environ["CHUNK_SIZE"] = str(parsed_args.chunk_size)
    if parsed_args.chunk_overlap:
        os.environ["CHUNK_OVERLAP"] = str(parsed_args.chunk_overlap)
    if parsed_args.collection_name:
        os.environ["QDRANT_COLLECTION_NAME"] = parsed_args.collection_name
    if parsed_args.cohere_model:
        os.environ["COHERE_MODEL"] = parsed_args.cohere_model

    # Set logging level based on verbose flag
    if parsed_args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    return main()


if __name__ == "__main__":
    result = main_with_args()
    print(f"Ingestion completed with result: {result}")
